{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Computer Vision\nDuring the first half of this notebook, we'll walk through some basic image loading and pre-processing techniques in Python. Then, we'll plug into Microsoft Azure to play with some of their AI tools."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Manipulating Images\nWhen a computer looks at an image, it doesn't see shapes and objects. Instead, it sees grids of pixels. AI models are able to process these numerical values to achieve some pretty amazing tasks."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Loading an Image\nWe'll start by loading a .JPG image file. Run the following cell of code to load and display the image."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport numpy as np\n%matplotlib inline\n\nimg_url = 'https://thumbor.forbes.com/thumbor/fit-in/416x416/filters%3Aformat%28jpg%29/https%3A%2F%2Fspecials-images.forbesimg.com%2Fimageserve%2F5d8acdb96de3150009a5060c%2F0x0.jpg%3Fbackground%3D000000%26cropX1%3D1338%26cropX2%3D3471%26cropY1%3D239%26cropY2%3D2372'\n\n# download the image and display it\nresponse = requests.get(img_url)\nimg = Image.open(BytesIO(response.content))\nplt.imshow(img);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This is what an image looks like to us as humans.\n\n### Examine the Image\nNow let's see how the computer sees the image. Run the cell below to convert the image to a **matrix** (a grid of numbers)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "img = np.array(img)\nimg",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now the image is represented as a grid of pixel values. Keep in mind, since there are so many values our code doesn't print them all. As a human, this looks like nothing more than gibberish, however, computers are great at finding patterns in the raw pixels that can be used to detect similar objects. \n\nNext let's find the pixel dimensions of the image."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "(height, width, color_channels) = img.shape\nprint(\"height = \" + str(height))\nprint(\"width = \" + str(width))\nprint(\"color channels = \" + str(color_channels))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can see that the image is 416 pixels tall by 416 pixels wide. We also see that the image has 3 color channels, which is another way for saying we have 3 color values for each pixel: <span style=\"color: red;\">Red</span>, <span style=\"color: green;\">Green</span>, and <span style=\"color: blue;\">Blue</span>.\n\nLet's take a look at a pixel in the image."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pixel_x = 200\npixel_y = 350\npixel_values = img[pixel_y, pixel_x, :]\npixel_values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<p>Here, we can see that the pixel at (200, 350) has a RGB color value of: (51, 92, 216)</p>\n<p>Earlier we discussed that in computer vision, every pixel value is a <b>feature</b>. Let's see how many features we have in our image.</p>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "num_features = height * width * color_channels\nprint(str(height) + \" x \" + str(width) + \" x \" + str(color_channels) + \" = \" + str(num_features) + \" total features!\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "That is a <b>LOT</b> of features for our model to process! Let's see if we can use some pre-processing techniques to shrink that down a bit."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Image Pre-Processing\nToday we learned that it's sometimes helpful to pre-process images before feeding them to our model. Pre-processing is a way to manipulate an image in a way that makes it easier for a model to learn from."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Grayscaling\nOne of the first (and most basic) pre-processing techniques we discussed was grayscaling. By removing color from an image, we significantly reduce the feature size of the image. Let's do this by selecting only the first color channel from out original image."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "grayscaled = img[:,:,0]\nplt.imshow(grayscaled, cmap='gray')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Great! Our image now has 1 color channel instead of 3. Now let's calculate the total number of features again."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "num_features = height * width\nprint(str(height) + \" x \" + str(width) + \" = \" + str(num_features) + \" total features!\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Nice! We were able to reduce our total feature size by 2/3! Keep in mind, however, that in some problems we will <i>need</i> to keep color information. For instance, if we were building an image classifier to detect dog breeds, it would be very difficult to tell the difference between a brown dog and a black dog from only a grayscale image."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Normalizing\nAnother pre-processing technique we talked about is normalization. Machine learning models tend to learn much easier if all of the input data is in the same range (usually 0-1).\n\nWe know that pixel values range from 0-255. Let's print out the pixel values and take a look."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "img",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Yep, each pixel value seems to be in the range of 0-255. In order to normalize these values, we can simply divide every pixel value by 255, which is the maximum."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "normalized_img = img / 255.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<p>Let's take a look at the values again to make sure they are in the range of 0-1.</p>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "normalized_img",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Great! Our image is now properly normalized."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Cropping\nThe final pre-processing step we discussed is cropping. A lot of times, models can become confused by unimportant background objects. An easy way to combat this is to properly crop only the perintent information in an object. For instance, if we were building a face recognition algorithm, we would want to crop just the face so that we are sure our model doesn't learn anything from the surrounding objects in the image."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Luckily, we can easily crop an image by specifying the coordinates of the desired region. Below we do this by specifying the min/max X values of the cropped image, as well as the min/max Y values."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "min_x = 150\nmin_y = 100\nmax_x = 270\nmax_y = 210\ncropped = img[min_y:max_y, min_x:max_x, :]\nplt.imshow(img)\nplt.imshow(cropped)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Though we had to manually select the exact coordinates of the crop, we will soon use tools from Microsoft that can automatically detect and crop faces for us."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Image Augmentation\nSometimes we may want to train an image classifier but find ourselves unable to get our hands on a large image dataset. Luckily, we can use **image augmentation** to generate a bunch of different variations of our data. This provides the model with more training examples with different lighting conditions and orientations. \n\nAgain, we'll load our original colored image."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nfrom PIL import Image, ImageEnhance\nimport requests\nfrom io import BytesIO\nimport numpy as np\n%matplotlib inline\n\nimg_url = 'https://thumbor.forbes.com/thumbor/fit-in/416x416/filters%3Aformat%28jpg%29/https%3A%2F%2Fspecials-images.forbesimg.com%2Fimageserve%2F5d8acdb96de3150009a5060c%2F0x0.jpg%3Fbackground%3D000000%26cropX1%3D1338%26cropX2%3D3471%26cropY1%3D239%26cropY2%3D2372'\n\n# download the image and display it\nresponse = requests.get(img_url)\nimg = Image.open(BytesIO(response.content))\nplt.imshow(img);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's rotate the image 30 degrees to produce a new image."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "rotated = img.rotate(30)\nplt.imshow(rotated);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, we'll create a new, brighter image."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "enhancer = ImageEnhance.Brightness(img)\nbright = enhancer.enhance(3)\nplt.imshow(bright)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally, we'll flip the image horizontally."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "flipped = np.flip(img, 1)\nplt.imshow(flipped)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "By combining these methods, we can randomly generate a bunch of variations of an image. Let's test this out by generating some examples!"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "rows = 4\ncolumns = 4\nfig = plt.figure(figsize=(8, 8))\nfor i in range(1, rows*columns + 1):\n    orig_img = img.copy()\n    shape = np.array(orig_img).shape\n    \n     # randomly brighten/darken the image\n    brightness = np.random.uniform(0.1, 5, size=1)[0]\n    enhancer = ImageEnhance.Brightness(orig_img)\n    orig_img = enhancer.enhance(brightness)\n    \n    # randomly rotate the image\n    rotation = np.random.randint(45, size=1)[0]\n    orig_img = orig_img.rotate(rotation)\n    \n    # randomly flip the image\n    shouldFlip = np.random.randint(2, size=1)[0]\n    if shouldFlip:\n        orig_img = np.flip(orig_img, 1)\n        \n    fig.add_subplot(rows, columns, i)\n    plt.imshow(orig_img)\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We have now seen how we can randomly transform an image to expand our dataset. There are many other transformations we could use to have an even more diverse dataset."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Microsoft Azure's Cognitive Services"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\n1. Open another browser tab and navigate to https://portal.azure.com.\n2. Sign in using your Microsoft account.\n3. Click **+ Create a resource**, and search **Computer Vision**.\n4. Click on **Computer Vision** and in the **Create** blade, enter the following details, and then click **Create**\n  * **Name**: A unique name for your service.\n  * **Subscription**: Azure for Students\n  * **Location**: (US) South Central US\n  * **Pricing tier**: Choose the F0 pricing tier.\n  * **Resource Group**: Choose the existing resource group you created in the previous lab. If you didn't complete the previous lab, click on **Create new**, enter a unique name, and click **Ok** to create a new resource group.)\n5. Wait for the service to be created and when the deployment is complete, click **Go to resource**.\n6. In your resource's blade, click **Keys and Endpoint** and then copy **Key 1** to the clipboard and paste it into the **visionKey** variable assignment value in the cell below. \n7. Run the cell below to assign the variables."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "visionKey = 'YOUR_KEY'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Loading an Image\nLet's start with the same image we analyzed previously.\n\nRun the code in the cell below to retrieve the original colored image from the URL:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "try:\n    visionKey\n    import matplotlib.pyplot as plt\n    from PIL import Image\n    import numpy as np\n    import requests\n    from io import BytesIO\n    %matplotlib inline\n\n    img_url = 'https://thumbor.forbes.com/thumbor/fit-in/416x416/filters%3Aformat%28jpg%29/https%3A%2F%2Fspecials-images.forbesimg.com%2Fimageserve%2F5d8acdb96de3150009a5060c%2F0x0.jpg%3Fbackground%3D000000%26cropX1%3D1338%26cropX2%3D3471%26cropY1%3D239%26cropY2%3D2372'\n\n    # download the image and display it\n    response = requests.get(img_url)\n    img = Image.open(BytesIO(response.content))\n    plt.imshow(img);\nexcept:\n    print(\"Make sure you run the code cell above that assigns the visionKey variable! Then run this cell again.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Use the Computer Vision API to Get Image Features\nThe Computer Vision API uses a machine learning model that has been pre-trained on millions of images.\n\nRun the cel below to what caption the Computer Vision API suggests for the imagae above."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "visionURI = 'SouthCentralUS.api.cognitive.microsoft.com'\n\ndef get_image_features(img_url):\n    import http.client, urllib.request, urllib.parse, urllib.error, base64, json\n\n    headers = {\n        # Request headers.\n        'Content-Type': 'application/json',\n        'Ocp-Apim-Subscription-Key': visionKey,\n    }\n\n    params = urllib.parse.urlencode({\n        # Request parameters. All of them are optional.\n        'visualFeatures': 'Categories,Description,Color',\n        'language': 'en',\n    })\n\n    body = \"{'url':'\" + img_url + \"'}\"\n\n    try:\n        # Execute the REST API call and get the response.\n        conn = http.client.HTTPSConnection(visionURI)\n        conn.request(\"POST\", \"/vision/v1.0/analyze?%s\" % params, body, headers)\n        response = conn.getresponse()\n        data = response.read()\n\n        # 'data' contains the JSON response.\n        parsed = json.loads(data.decode(\"UTF-8\"))\n        if response is not None:\n            return parsed\n        conn.close()\n\n\n    except Exception as e:\n        print('Error:')\n        print(e)\n        \njsonData = get_image_features(img_url)\ndesc = jsonData['description']['captions'][0]['text']\nprint(desc)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The description is reasonably, if not exactly, appropriate.\n\nRun the cell below to see the full JSON response, including image properties and suggested tags."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import json\n\n# View the full details returned\nprint (json.dumps(jsonData, sort_keys=True, indent=2))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's try with a different image:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Get the image and show it\nimg_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/uke.jpg'\n\n# download the image and display it\nresponse = requests.get(img_url)\nimg = Image.open(BytesIO(response.content))\nplt.imshow(img)\nplt.show()\njsonData = get_image_features(img_url)\ndesc = jsonData['description']['captions'][0]['text']\nprint(desc)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "How about something a little more complex?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Get the image and show it\nimg_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/soccer.jpg'\n\n# download the image and display it\nresponse = requests.get(img_url)\nimg = Image.open(BytesIO(response.content))\nplt.imshow(img)\nplt.show()\njsonData = get_image_features(img_url)\ndesc = jsonData['description']['captions'][0]['text']\nprint(desc)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Using the Face API\nWhile the Computer Vision API is useful for general image analysis, the Face API offers specific functions for analyzing faces in images. This can be useful in a variety of AI scenarios.\n\n### Create a Face API Service\nTo provision a Computer Vision API service in your Azure subscription, Follow these steps:\n\n1. Open another browser tab and navigate to https://portal.azure.com.\n2. Sign in using your Microsoft account.\n3. Click **+ Create a Resource**, and search **Face**.\n4. Click **Face** and in the **Create** blade, enter the following details, and then click **Create**\n  * **Name**: A unique name for your service.\n  * **Subscription**: Azure for Students\n  * **Location**: (US) South Central US\n  * **Pricing tier**: Choose the F0 pricing tier.\n  * **Resource Group**: Choose the existing resource group you created earlier.\n5. Wait for the service to be created.\n6. When deployment is complete, click **Go to Resource**.\n7. In your resource's blade, click **Keys and Endpoint** and then copy **Key 1** to the clipboard and paste it into the **faceKey** variable assignment value in the cell below.\n8. Run the cell below to assign the variables."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "faceKey = \"YOUR_KEY\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The Face API has a Python SDK, which you can install as a package. This makes it easier to work with.\n\nRun the following cell to install the Face SDK."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "try:\n    faceKey\n    !pip install cognitive_face\nexcept:\n    print(\"Make sure you run the code cell above that assigns the faceKey variable! Then run this cell again.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now you're ready to use the Face API. First, let's see if we can detect a face in an image:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import cognitive_face as CF\n\nfaceURI = \"https://SouthCentralUS.api.cognitive.microsoft.com/face/v1.0/\"\n\n# Set URI and Key\nCF.BaseUrl.set(faceURI)\nCF.Key.set(faceKey)\n\n\n# Detect faces in an image\nimg_url = 'https://thumbor.forbes.com/thumbor/fit-in/416x416/filters%3Aformat%28jpg%29/https%3A%2F%2Fspecials-images.forbesimg.com%2Fimageserve%2F5d8acdb96de3150009a5060c%2F0x0.jpg%3Fbackground%3D000000%26cropX1%3D1338%26cropX2%3D3471%26cropY1%3D239%26cropY2%3D2372'\nresult = CF.face.detect(img_url)\nprint (result)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The Face API has detected one face, and assigned it an ID. It also returns the coordinates for the top left corner and the width and height for the rectangle within which the face is detected.\n\nRun the cell below to show the rectange on the image."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport requests\nfrom io import BytesIO\nfrom matplotlib.pyplot import imshow\nfrom PIL import Image, ImageDraw\n\n# Get the image\nresponse = requests.get(img_url)\nimg = Image.open(BytesIO(response.content))\n\n# Add rectangles for each face found\ncolor=\"red\"\nif result is not None:\n    draw = ImageDraw.Draw(img) \n    for currFace in result:\n        faceRectangle = currFace['faceRectangle']\n        left = faceRectangle['left']\n        top = faceRectangle['top']\n        width = faceRectangle['width']\n        height = faceRectangle['height']\n        draw.line([(left,top),(left+width,top)],fill=color, width=5)\n        draw.line([(left+width,top),(left+width,top+height)],fill=color , width=5)\n        draw.line([(left+width,top+height),(left, top+height)],fill=color , width=5)\n        draw.line([(left,top+height),(left, top)],fill=color , width=5)\n\n# show the image\nimshow(img)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As well as detecting the face, the Face API assigned an ID to this face. The ID is retained by the service for a while, enabling you to reference it. Run the following cell to see the ID assigned to the face that has been detected:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "face1 = result[0]['faceId']\nprint (\"Face 1:\" + face1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "One useful thing you can do with the face ID is, is to use it to compare another image and see if a matching face is found. This kind of facial comparison is common in a variety of security / user authentication scenarios.\n\nLet's try it with another image of the same person:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Get the image to compare\nimg2_url = 'http://markcubancompanies.com/images/headshot3.jpg'\nresponse2 = requests.get(img2_url)\nimg2 = Image.open(BytesIO(response2.content))\n\n# Detect faces in a comparison image\nresult2 = CF.face.detect(img2_url)\n\n# Assume the first face is the one we want to compare\nif result2 is not None:\n    face2 = result2[0]['faceId']\n    print (\"Face 2:\" + face2)\n\ndef verify_face(face1, face2):\n    # By default, assume the match is unverified\n    verified = \"Not Verified\"\n    color=\"red\"\n\n    # compare the comparison face to the original one we retrieved previously\n    verify = CF.face.verify(face1, face2)\n\n    # if there's a match, set verified and change color to green\n    if verify['isIdentical'] == True:\n        verified = \"Verified\"\n        color=\"lightgreen\"\n\n    # Display the second face with a red rectange if unverified, or green if verified\n    draw = ImageDraw.Draw(img2) \n    for currFace in result2:\n        faceRectangle = currFace['faceRectangle']\n        left = faceRectangle['left']\n        top = faceRectangle['top']\n        width = faceRectangle['width']\n        height = faceRectangle['height']\n        draw.line([(left,top),(left+width,top)] , fill=color, width=5)\n        draw.line([(left+width,top),(left+width,top+height)] , fill=color, width=5)\n        draw.line([(left+width,top+height),(left, top+height)] , fill=color, width=5)\n        draw.line([(left,top+height),(left, top)] , fill=color, width=5)\n\n    # show the image\n    imshow(img2)\n\n    # Display verification status and confidence level\n    print(verified)\n    print (\"Confidence Level: \" + str(verify['confidence']))\n\nverify_face(face1, face2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "What if we try to match the original face to a different person?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Get the image to compare\nimg2_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/satya.jpg'\nresponse2 = requests.get(img2_url)\nimg2 = Image.open(BytesIO(response2.content))\n\n# Detect faces in a comparison image\nresult2 = CF.face.detect(img2_url)\n\n# Assume the first face is the one we want to compare\nface2 = result2[0]['faceId']\nprint (\"Face 2:\" + face2)\n\nverify_face(face1, face2)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "No match!"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Using the Custom Vision Cognitive Service\nThe *Custom Vision* cognitive service enables you to create custom computer vision solutions.\n\nIn this notebook, you will create and train a Custom Vision *image classification* project that can identify pictures of apples and carrots, and use it to classify new images.\n\n&gt; **Note**: *Some of the images used in the lab are sourced from the free image library at <a>www.pachd.com</a>*\n\n### Install the Custom Vision SDK\nThe first step is to install the Python SDK for the Custom Vision service:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Install the Custom Vision SDK\n! pip install azure-cognitiveservices-vision-customvision",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now lets download and extract the images you will use to train your classifier."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/produce.zip -o produce.zip\n!unzip produce.zip",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create a Custom Vision service instance\nNow you're ready to use the Custom Vision service. You'll need to create an instance of the service and get your unique training and prediction keys so you can access it:\n1. Go to https://customvision.ai/ and sign in using the Microsoft account associated with your Azure subscription.\n2. Click the *Settings* (⚙) icon at the top right and click **create new** to create a new resource.\n3. Enter the following information and click **Create resource**:\n  * **Name**: A unique name for your service.\n  * **Subscription**: Azure for Students\n  * **Resource Group**: Choose the existing resource group you created earlier.\n  * **Kind**: CognitiveServices\n  * **Location**: (US) South Central US\n  * **Pricing tier**: Choose the S0 pricing tier.\n4. Wait for the resource to be created.\n5. Copy *Key* to **customVisionKey**, *Endpoint* to **endpoint**, and *Resource Id* to **resourceID* in variables below and **run the cell**:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "customVisionKey = 'YOUR_KEY'\nendpoint = 'YOUR_ENDPOINT'\nresourceID = 'YOUR_RESOURCE_ID'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create a Custom Vision project\nNow we'll create a project for the apple/carrot classifier:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "try:\n    customVisionKey\n    endpoint\n    resourceID\n    \n    from azure.cognitiveservices.vision.customvision.training import CustomVisionTrainingClient\n\n    trainer = CustomVisionTrainingClient(customVisionKey, endpoint=endpoint)\n\n    # Create a new project\n    print (\"Creating project...\")\n    project = trainer.create_project(\"Produce Classification\")\n    print(\"The project was created!\")\nexcept:\n    print(\"Make sure you run the code cell above that assigns the customVisionKey, endpoint, and resourceID variables! Then run this cell again.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Add tags\nThe project will identify images as apples or carrots, so we'll need tags for those classes:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Make two tags in the new project\nprint(\"Creating tags...\")\napple_tag = trainer.create_tag(project.id, \"Apple\")\ncarrot_tag = trainer.create_tag(project.id, \"Carrot\")\nprint('Created tags!')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Upload training images\nNow that we've got the tags, we need to upload some images of apples and carrots, assign the appropriate tags:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\n\nprint(\"Adding images...\")\n\napples_dir = \"apples\"\nfor image in os.listdir(apples_dir):\n    with open(os.path.join(apples_dir, image), mode=\"rb\") as img_data: \n        trainer.create_images_from_data(project.id, img_data.read(), [apple_tag.id])\n\ncarrots_dir = \"carrots\"\nfor image in os.listdir(carrots_dir):\n    with open(os.path.join(carrots_dir, image), mode=\"rb\") as img_data: \n        trainer.create_images_from_data(project.id, img_data.read(), [carrot_tag.id])\n        \nprint('Added images!')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Return to your Custom Vision service and click the *Home* (⌂) icon to return to the home page, and then open the ***Apple or Carrot*** project to view the images that have been uploaded and tagged.\n\n### Train the project\nWith the tagged images in place, we're now ready to train a classification model:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import time\n\nprint (\"Training...\")\n# Train the project, checking status every 1 second\niteration = trainer.train_project(project.id)\nwhile (iteration.status == \"Training\"):\n    iteration = trainer.get_iteration(project.id, iteration.id)\n    print (\"Training status: \" + iteration.status)\n    time.sleep(1)\n\n# The iteration is now trained. Publish it to the project endpoint\ntrainer.publish_iteration(project.id, iteration.id, \"First Iteration\", resourceID)\n\n# Make it the default iteration\niteration = trainer.update_iteration(project_id= project.id, iteration_id=iteration.id, name= \"First Iteration\", is_default=True)\n\nprint (\"Trained!\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Use the project to classify images\nNow that we have a trained project, we can use it to predict the class of new images that weren't in the training dataset:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n%matplotlib inline\n\n# Use two test images\ntest_img1_url = 'http://www.pachd.com/free-images/food-images/apple-01.jpg'\ntest_img2_url = 'http://www.pachd.com/free-images/food-images/carrot-02.jpg'\n\ntest_image_urls = []\ntest_image_urls.append(test_img1_url)\ntest_image_urls.append(test_img2_url)\n\n# Create an instance of the prediction service\npredictor = CustomVisionPredictionClient(customVisionKey, endpoint=endpoint)\n\n# Create a figure\nfig = plt.figure(figsize=(16, 8))\n\n# Get the images and show the predicted classes\nfor url_idx in range(len(test_image_urls)):\n    response = requests.get(test_image_urls[url_idx])\n    image_contents = Image.open(BytesIO(response.content))\n    results = predictor.classify_image_url(project_id=project.id, published_name=iteration.name, url=test_image_urls[url_idx])\n    # The results include a prediction for each tag, in descending order of probability - so we'll get the first one\n    prediction = results.predictions[0].tag_name + \": {0:.2f}%\".format(results.predictions[0].probability * 100)\n    # Subplot for image and its predicted class\n    a=fig.add_subplot(1,2,url_idx+1)\n    imgplot = plt.imshow(image_contents)\n    a.set_title(prediction)\n\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Want to go a step further? Try testing your model on new images from Google! To do this, replace **test_img1_url** and **test_img2_url** in the code above with the URLs of the images you would like to test. Then re-run the code cell."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "# Nice job!\nYou have now learned how to use Microsoft Azure's computer vision tools to process images. In the next lab, you'll see how you can create a Custom Vision image classifier to recognize traffic signs."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}